{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "(HW3)=\n",
        "# HW3\n",
        "\n",
        "## Task 3.1 (0.5 points)\n",
        "\n",
        "Express $\\sigma''(x)$ in terms of $\\sigma(x)$ where $\\sigma(x) = \\frac 1{1 + e^{-t}}$ â€” sigmoid function. Find all $x$ for which $\\sigma''(x) = 0$. Also find intervals on which sigmoid is convex/concave.\n",
        "\n",
        "### YOUR SOLUTION HERE\n",
        "### Solution\n",
        "\n",
        "To express $\\sigma''(x)$ in terms of $\\sigma(x) = \\frac{1}{1 + e^{-x}}$, we first need to find the first and second derivatives of $\\sigma(x)$.\n",
        "\n",
        "Given that $\\sigma(x) = \\frac{1}{1 + e^{-x}}$, the first derivative, $\\sigma'(x)$, can be calculated as:\n",
        "\n",
        "$$\n",
        "\\sigma'(x) = \\frac{d}{dx} \\left( \\frac{1}{1 + e^{-x}} \\right) = \\frac{e^{-x}}{(1 + e^{-x})^2}\n",
        "$$\n",
        "\n",
        "Now, let's find the second derivative, $\\sigma''(x)$:\n",
        "\n",
        "$$\n",
        "\\sigma''(x) = \\frac{d}{dx} \\left( \\frac{e^{-x}}{(1 + e^{-x})^2} \\right)\n",
        "$$\n",
        "\n",
        "To compute this derivative, we can use the quotient rule:\n",
        "\n",
        "$$\n",
        "\\sigma''(x) = \\frac{(1 + e^{-x})^2 \\cdot (-e^{-x}) - e^{-x} \\cdot 2(1 + e^{-x}) \\cdot (-e^{-x})}{(1 + e^{-x})^4}\n",
        "$$\n",
        "\n",
        "Simplifying the expression yields:\n",
        "\n",
        "$$\n",
        "\\sigma''(x) = \\frac{e^{-x} (e^{-x} - 1)}{(1 + e^{-x})^3}\n",
        "$$\n",
        "\n",
        "Now, to find all $x$ for which $\\sigma''(x) = 0$, we set $\\sigma''(x)$ equal to zero and solve for $x$:\n",
        "\n",
        "$$\n",
        "e^{-x} (e^{-x} - 1) = 0\n",
        "$$\n",
        "\n",
        "This equation is satisfied when $e^{-x} = 0$ or $e^{-x} - 1 = 0$. However, $e^{-x}$ is never equal to zero for real $x$, so we focus on solving $e^{-x} - 1 = 0$:\n",
        "\n",
        "$$\n",
        "e^{-x} - 1 = 0 \\implies e^{-x} = 1 \\implies -x = 0 \\implies x = 0\n",
        "$$\n",
        "\n",
        "Therefore, the only solution for $\\sigma''(x) = 0$ is $x = 0$.\n",
        "\n",
        "To determine the intervals on which the sigmoid function is convex or concave, we need to analyze the sign of $\\sigma''(x)$. \n",
        "\n",
        "Since $\\sigma''(x) = \\frac{e^{-x} (e^{-x} - 1)}{(1 + e^{-x})^3}$, we can observe that:\n",
        "\n",
        "- $e^{-x} > 0$ and $(e^{-x} - 1) > 0$ for all real $x$.\n",
        "- $(1 + e^{-x})^3$ is always positive.\n",
        "\n",
        "Thus, the sign of $\\sigma''(x)$ depends solely on the sign of $e^{-x} (e^{-x} - 1)$. \n",
        "\n",
        "- When $e^{-x} (e^{-x} - 1) > 0$, $\\sigma''(x) > 0$, indicating that the function is convex.\n",
        "- When $e^{-x} (e^{-x} - 1) < 0$, $\\sigma''(x) < 0$, indicating that the function is concave.\n",
        "\n",
        "Since $e^{-x} (e^{-x} - 1)$ is positive for $0 < x < \\infty$ and negative for $-\\infty < x < 0$, the sigmoid function is convex for $0 < x < \\infty$ and concave for $-\\infty < x < 0$.function $\\sigma(x)$ is concave everywhere.\n"
      ],
      "metadata": {},
      "id": "bf0d66b6"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 3.2 (0.5 points)\n",
        "\n",
        "Let $\\boldsymbol X \\in \\mathbb R^{m\\times n}$ and $f(\\boldsymbol X) = \\vert \\boldsymbol X \\vert^k$, $k\\in\\mathbb N$. Find $\\nabla f(\\boldsymbol X)$.\n",
        "\n",
        "### YOUR SOLUTION HERE\n",
        "### Solution\n",
        "\n",
        "To find the gradient of $f(\\boldsymbol X)$, we first need to express $f(\\boldsymbol X)$ in terms of its components.\n",
        "\n",
        "Given that $f(\\boldsymbol X) = |\\boldsymbol X|^k$, where $\\boldsymbol X$ is a matrix with elements $x_{ij}$, the absolute value of $\\boldsymbol X$ is defined as the matrix whose elements are the absolute values of the corresponding elements of $\\boldsymbol X$. Thus, $|\\boldsymbol X|$ is also a matrix with elements $|x_{ij}|$.\n",
        "\n",
        "To compute the gradient $\\nabla f(\\boldsymbol X)$, we need to differentiate $f(\\boldsymbol X)$ with respect to each element $x_{ij}$ of $\\boldsymbol X$.\n",
        "\n",
        "Let's denote the $i$th row and $j$th column of $\\boldsymbol X$ as $[\\boldsymbol X]_{i,j}$. Then, we have:\n",
        "\n",
        "$$\n",
        "f(\\boldsymbol X) = |[\\boldsymbol X]_{i,j}|^k\n",
        "$$\n",
        "\n",
        "Using the chain rule for matrix derivatives, we can write the gradient as:\n",
        "\n",
        "$$\n",
        "\\nabla f(\\boldsymbol X) = k |[\\boldsymbol X]_{i,j}|^{k-1} \\cdot \\text{sgn}([\\boldsymbol X]_{i,j}) \\cdot \\boldsymbol E_{i,j}\n",
        "$$\n",
        "\n",
        "where $\\text{sgn}(x)$ is the sign function, returning $1$ if $x > 0$, $-1$ if $x < 0$, and $0$ if $x = 0$, and $\\boldsymbol E_{i,j}$ is the matrix with zeros everywhere except for a $1$ at position $(i, j)$.\n",
        "\n",
        "Thus, the gradient $\\nabla f(\\boldsymbol X)$ is a matrix of the same size as $\\boldsymbol X$, where each element is $k |x_{ij}|^{k-1} \\cdot \\text{sgn}(x_{ij})$."
      ],
      "metadata": {},
      "id": "cebd2b22"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 3.3 (2 points)\n",
        "\n",
        "Let $\\boldsymbol A \\in \\mathbb R^{n\\times n}$ be a symmetric matrix and\n",
        "\n",
        "$$\n",
        "f(\\boldsymbol x) = \\frac{\\boldsymbol x^\\mathsf{T} \\boldsymbol{Ax}}{\\boldsymbol x^\\mathsf{T} \\boldsymbol x}, \\quad \\boldsymbol x \\in \\mathbb R^n.\n",
        "$$ \n",
        "\n",
        "(a) (**1 point**) Calculate $\\nabla f(\\boldsymbol x)$ and prove that $\\nabla f(\\boldsymbol x) = \\boldsymbol 0$ iff $\\boldsymbol x$ is an eigenvector of $\\boldsymbol A$.\n",
        "\n",
        "(b) (**1 point**) Prove that if $\\boldsymbol A$ is positive definite then\n",
        "\n",
        "$$\n",
        "\\max\\limits_{\\boldsymbol x \\ne \\boldsymbol 0} f(\\boldsymbol x) = \\lambda_{\\max}(\\boldsymbol A), \\quad\n",
        "\\min\\limits_{\\boldsymbol x \\ne \\boldsymbol 0} f(\\boldsymbol x) = \\lambda_{\\min}(\\boldsymbol A)\n",
        "$$\n",
        "\n",
        "### YOUR SOLUTION HERE\n",
        "\n",
        "\n",
        "\n",
        "### Solution\n",
        "\n",
        "(a) To calculate $\\nabla f(\\boldsymbol{x})$, let's first express $f(\\boldsymbol{x})$ in terms of its components. We have:\n",
        "\n",
        "$$\n",
        "f(\\boldsymbol{x}) = \\frac{\\boldsymbol{x}^\\mathsf{T} \\boldsymbol{Ax}}{\\boldsymbol{x}^\\mathsf{T} \\boldsymbol{x}}\n",
        "$$\n",
        "\n",
        "Expanding the numerator and denominator, we get:\n",
        "\n",
        "$$\n",
        "f(\\boldsymbol{x}) = \\frac{\\sum_{i,j} x_i a_{ij} x_j}{\\sum_{i,j} x_i x_j}\n",
        "$$\n",
        "\n",
        "To find the gradient $\\nabla f(\\boldsymbol{x})$, we differentiate $f(\\boldsymbol{x})$ with respect to each element $x_k$ of $\\boldsymbol{x}$. Let's denote the $k$th component of $\\boldsymbol{x}$ as $x_k$. Then, we have:\n",
        "\n",
        "$$\n",
        "\\nabla f(\\boldsymbol{x}) = \\left( \\frac{\\partial f}{\\partial x_1}, \\frac{\\partial f}{\\partial x_2}, \\ldots, \\frac{\\partial f}{\\partial x_n} \\right)\n",
        "$$\n",
        "\n",
        "We differentiate $f(\\boldsymbol{x})$ with respect to $x_k$ as follows:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial f}{\\partial x_k} = \\frac{\\frac{\\partial}{\\partial x_k} \\left( \\sum_{i,j} x_i a_{ij} x_j \\right) \\sum_{i,j} x_i x_j - \\sum_{i,j} x_i a_{ij} x_j \\frac{\\partial}{\\partial x_k} \\left( \\sum_{i,j} x_i x_j \\right)}{\\left( \\sum_{i,j} x_i x_j \\right)^2}\n",
        "$$\n",
        "\n",
        "After some algebraic manipulation, we can simplify this expression and express it in terms of the elements of $\\boldsymbol{A}$ and $\\boldsymbol{x}$. We find that:\n",
        "\n",
        "$$\n",
        "\\nabla f(\\boldsymbol{x}) = \\frac{2\\boldsymbol{Ax} - 2\\lambda\\boldsymbol{x}}{\\boldsymbol{x}^\\mathsf{T} \\boldsymbol{x}}\n",
        "$$\n",
        "\n",
        "where $\\lambda$ is the eigenvalue corresponding to the eigenvector $\\boldsymbol{x}$.\n",
        "\n",
        "To prove that $\\nabla f(\\boldsymbol{x}) = \\boldsymbol{0}$ if and only if $\\boldsymbol{x}$ is an eigenvector of $\\boldsymbol{A}$, let's set $\\nabla f(\\boldsymbol{x}) = \\boldsymbol{0}$ and solve for $\\boldsymbol{x}$. If $\\nabla f(\\boldsymbol{x}) = \\boldsymbol{0}$, then:\n",
        "\n",
        "$$\n",
        "\\frac{2\\boldsymbol{Ax} - 2\\lambda\\boldsymbol{x}}{\\boldsymbol{x}^\\mathsf{T} \\boldsymbol{x}} = \\boldsymbol{0}\n",
        "$$\n",
        "\n",
        "This equation implies that $2\\boldsymbol{Ax} - 2\\lambda\\boldsymbol{x} = \\boldsymbol{0}$, which simplifies to $\\boldsymbol{Ax} = \\lambda\\boldsymbol{x}$. This is the definition of an eigenvector, so we have proved the forward implication.\n",
        "\n",
        "For the reverse implication, if $\\boldsymbol{x}$ is an eigenvector of $\\boldsymbol{A}$, then $\\boldsymbol{Ax} = \\lambda\\boldsymbol{x}$ for some scalar $\\lambda$. Substituting this into the expression for $\\nabla f(\\boldsymbol{x})$, we find that $\\nabla f(\\boldsymbol{x}) = \\boldsymbol{0}$. Therefore, we have proved that $\\nabla f(\\boldsymbol{x}) = \\boldsymbol{0}$ if and only if $\\boldsymbol{x}$ is an eigenvector of $\\boldsymbol{A}$.\n",
        "\n",
        "(b) If $\\boldsymbol{A}$ is positive definite, it means that all its eigenvalues are positive. Let $\\boldsymbol{v}$ be an eigenvector of $\\boldsymbol{A}$ corresponding to the eigenvalue $\\lambda_{\\max}(\\boldsymbol{A})$, the maximum eigenvalue of $\\boldsymbol{A}$. Then, we have:\n",
        "\n",
        "$$\n",
        "f(\\boldsymbol{v}) = \\frac{\\boldsymbol{v}^\\mathsf{T} \\boldsymbol{Av}}{\\boldsymbol{v}^\\mathsf{T} \\boldsymbol{v}} = \\frac{\\lambda_{\\max}(\\boldsymbol{A})\\boldsymbol{v}^\\mathsf{T} \\boldsymbol{v}}{\\boldsymbol{v}^\\mathsf{T} \\boldsymbol{v}} = \\lambda_{\\max}(\\boldsymbol{A})\n",
        "$$\n",
        "\n",
        "Similarly, let $\\boldsymbol{u}$ be an eigenvector of $\\boldsymbol{A}$ corresponding to the eigenvalue $\\lambda_{\\min}(\\boldsymbol{A})$, the minimum eigenvalue of $\\boldsymbol{A}$. Then, we have:\n",
        "\n",
        "$$\n",
        "f(\\boldsymbol{u}) = \\frac{\\boldsymbol{u}^\\mathsf{T} \\boldsymbol{Au}}{\\boldsymbol{u}^\\mathsf{T} \\boldsymbol{u}} = \\frac{\\lambda_{\\min}(\\boldsymbol{A})\\boldsymbol{u}^\\mathsf{T} \\boldsymbol{u}}{\\boldsymbol{u}^\\mathsf{T} \\boldsymbol{u}} = \\lambda_{\\min}(\\boldsymbol{A})\n",
        "$$\n",
        "\n",
        "Since $\\boldsymbol{A}$ is positive definite, all its eigenvalues are positive, so $\\lambda_{\\max}(\\boldsymbol{A}) > 0$ and $\\lambda_{\\min}(\\boldsymbol{A}) > 0$. Therefore, $\\max\\limits_{\\boldsymbol{x} \\ne \\boldsymbol{0}} f(\\boldsymbol{x}) = \\lambda_{\\max}(\\boldsymbol{A})$ and $\\min\\limits_{\\boldsymbol{x} \\ne \\boldsymbol{0}} f(\\boldsymbol{x}) = \\lambda_{\\min}(\\boldsymbol{A})$."
      ],
      "metadata": {},
      "id": "e2b0a12c"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 3.4 (1 point)\n",
        "\n",
        "4. Find \n",
        "\n",
        "    $$\n",
        "        \\min\\limits_{\\boldsymbol X} \\Vert \\boldsymbol{AX} - \\boldsymbol B  \\Vert_F^2, \\quad \\boldsymbol A \\in \\mathbb R^{k\\times m},\\quad \\boldsymbol B \\in \\mathbb R^{k\\times n},\\quad \\mathrm{rank}(\\boldsymbol A) = m.\n",
        "    $$\n",
        "\n",
        "### YOUR SOLUTION  HERE\n",
        "### Solution\n",
        "\n",
        "To minimize $\\Vert \\boldsymbol{AX} - \\boldsymbol B  \\Vert_F^2$, where $\\boldsymbol{A} \\in \\mathbb{R}^{k \\times m}$, $\\boldsymbol{B} \\in \\mathbb{R}^{k \\times n}$, and $\\mathrm{rank}(\\boldsymbol{A}) = m$, we need to find the matrix $\\boldsymbol{X}$ that minimizes the Frobenius norm of the residual matrix $\\boldsymbol{AX} - \\boldsymbol{B}$.\n",
        "\n",
        "The Frobenius norm of a matrix $\\boldsymbol{M}$ is defined as $\\Vert \\boldsymbol{M} \\Vert_F = \\sqrt{\\sum_{i=1}^{m} \\sum_{j=1}^{n} |m_{ij}|^2}$, where $m_{ij}$ are the elements of $\\boldsymbol{M}$.\n",
        "\n",
        "The residual matrix $\\boldsymbol{R} = \\boldsymbol{AX} - \\boldsymbol{B}$ has dimensions $k \\times n$. Therefore, the Frobenius norm of $\\boldsymbol{R}$ is given by:\n",
        "\n",
        "$$\n",
        "\\Vert \\boldsymbol{R} \\Vert_F = \\sqrt{\\sum_{i=1}^{k} \\sum_{j=1}^{n} |r_{ij}|^2} = \\sqrt{\\sum_{i=1}^{k} \\sum_{j=1}^{n} |(\\boldsymbol{AX} - \\boldsymbol{B})_{ij}|^2}\n",
        "$$\n",
        "\n",
        "The goal is to minimize this norm by choosing an appropriate $\\boldsymbol{X}$. \n",
        "\n",
        "To find the optimal $\\boldsymbol{X}$, we can use the least squares solution, which minimizes the sum of the squares of the differences between the elements of $\\boldsymbol{AX}$ and $\\boldsymbol{B}$.\n",
        "\n",
        "The least squares solution for overdetermined systems (where $\\boldsymbol{A}$ has more rows than columns) is given by:\n",
        "\n",
        "$$\n",
        "\\boldsymbol{X} = (\\boldsymbol{A}^\\mathsf{T} \\boldsymbol{A})^{-1} \\boldsymbol{A}^\\mathsf{T} \\boldsymbol{B}\n",
        "$$\n",
        "\n",
        "Given that $\\mathrm{rank}(\\boldsymbol{A}) = m$, the matrix $\\boldsymbol{A}^\\mathsf{T} \\boldsymbol{A}$ is invertible.\n",
        "\n",
        "Therefore, the minimum of $\\Vert \\boldsymbol{AX} - \\boldsymbol{B}  \\Vert_F^2$ is achieved when $\\boldsymbol{X} = (\\boldsymbol{A}^\\mathsf{T} \\boldsymbol{A})^{-1} \\boldsymbol{A}^\\mathsf{T} \\boldsymbol{B}$."
      ],
      "metadata": {},
      "id": "e34afdb9"
    }
  ],
  "metadata": {
    "celltoolbar": "Tags",
    "kernelspec": {
      "name": "python",
      "language": "python",
      "display_name": "Pyolite (preview)"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    },
    "kernel_info": {
      "name": "python"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}