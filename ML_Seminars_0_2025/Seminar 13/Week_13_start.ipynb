{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7796dff",
   "metadata": {},
   "source": [
    " # Decion based models. From decision Tree to Random Forest\n",
    "\n",
    "### Load the California housing Dataset and Prepare the Data\n",
    "- Load the California housing dataset from sklearn (`sklearn.datasets.fetch_california_housing()`).\n",
    "- Separate the features and the target variable, which is 'medv' in this case.\n",
    "- Analyse the dataset and process different necessary transformations of the data.\n",
    "- Split the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1aabcf9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b4f1d53c",
   "metadata": {},
   "source": [
    "###  Fit the Regression Tree\n",
    "- Fit a decision tree to the training data.\n",
    "- Try to vizualize the behavior of the DT while changing parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfbb32c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d49b39c2",
   "metadata": {},
   "source": [
    "### Perform Cost Complexity Pruning\n",
    "- Apply cost complexity pruning to the regression tree using the appropriate library function : `cost_complexity_pruning_path()`\n",
    "- Determine the optimal pruning parameter `ccp_alpha` through cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a683869",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a7a3768b",
   "metadata": {},
   "source": [
    "### Analyze the results\n",
    "- Use the best estimator obtained from cross-validation to make predictions on the test set\n",
    "- Utilize the `predict()` method of the best estimator to generate predictions for the test data\n",
    "- Evaluate the Performance of the Model on the Test Set\n",
    "- Plot the tree to see if its interpretable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9508f383",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b85b1ced",
   "metadata": {},
   "source": [
    "### Some questions\n",
    "\n",
    "- When learning the tree, we chose a feature to test at each step by maximizing the expected information gain. Does this approach allow us to generate the optimal decision\n",
    "tree ? Why or why not ? Hint : When playing chess, do you consider only the immediate improvement of your position when deciding on your next move ?\n",
    "\n",
    "- Why might a Decision Tree work well on a small dataset but perform poorly on a larger one ?\n",
    "\n",
    "- How does the depth of the tree impact its performance in terms of overfitting and underfitting ?\n",
    "\n",
    "- How are categorical features handled during splitting ?\n",
    " \n",
    "- In your opinion, why is feature scaling not required when training a Decision Tree ?\n",
    "\n",
    "- When splitting a node, what happens if all observations have the same target value ?\n",
    "\n",
    "- What Decision Trees are often described as \"white-box models\". What does this mean, and why is it beneficial in certain applications ?\n",
    "\n",
    "- If your dataset is imbalanced, how might this affect the splits chosen by the Decision Tree ? What adjustments could you make to address this ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1d29f6",
   "metadata": {},
   "source": [
    "## Experiment Boostrapping\n",
    "\n",
    "- Write a function that creates bootstrap samples by randomly sampling data points with replacement from the training set\n",
    "- Train multiple Decision Tree models, each using a different bootstrap sample\n",
    "- Generate predictions from each model for a fixed test set and store the results\n",
    "- Compute the mean, standard deviation, and range of predictions for each test point across the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3bc694",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "35b74746",
   "metadata": {},
   "source": [
    "- Create some vizualisation to compare the mean predictions with the true values\n",
    "- Show the distribution of predictions for a few selected test points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb1e273",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8152e6f5",
   "metadata": {},
   "source": [
    "- How does the variance of predictions change across test samples ?\n",
    "- Why do some test points exhibit higher variance ?\n",
    "- How does this relate to the overfitting tendency of Decision Trees ?\n",
    "- How might combining predictions help to reduce variance ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d45ed5c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8d9c47f7",
   "metadata": {},
   "source": [
    "- Try using a different weak learners with the same setup and compare their prediction variance\n",
    "- Increase or decrease the number of bootstrap samples. How does this affect the results ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2338ffa2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5a997665",
   "metadata": {},
   "source": [
    "### Bagging Implementation\n",
    "\n",
    "- In order to improve the MSE and reduce the variance of the results implement your own Bagging class.\n",
    "\n",
    "- Recall that Bagging is performed when all predictors (i.e. covariates - features) are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860a8d83",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c7cf7777",
   "metadata": {},
   "source": [
    "- Fit your Bagging regressor to the training data. Compare it to `DecisionTree` Sklearn built-in class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d55135",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4a560ec5",
   "metadata": {},
   "source": [
    "- Effect of Increasing Trees in Bagging : \n",
    "    - Vizualize the effect of increasing the number of weak learners. \n",
    "    - Does increasing the number of trees always help ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121f95f3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5f090e88",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "Random Forest introduces additional randomness by restricting the number of features that each tree can consider when splitting a node.\n",
    "\n",
    "Usually, $\\sqrt{p}$ features for classification and $p/3$ for regression, where $p$ is the total number of features.\n",
    "\n",
    "This restriction decorrelates the trees, reducing the chance of overfitting even further.\n",
    "\n",
    "- Modify your class to chose the maximum number of features to consider when searching for the best split. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19eacfec",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9ea9193d",
   "metadata": {},
   "source": [
    "- Create a plot displaying the test, train and OOB error resulting from random forests for a more comprehensive range of values for max_features and n_estimators\n",
    "- Describe the results obtained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbf9873",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "736d69ed",
   "metadata": {},
   "source": [
    "- Visualize the difference in variance between your Decision Tree and Random Forest (which you can replicate with different parameters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127dd3f3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "24f21c31",
   "metadata": {},
   "source": [
    "--- "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
