{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SIS 2 (6 points)\n",
    "\n",
    "Deadline for submission : **Monday 24th March : 23h50**\n",
    "\n",
    "- The defense for this SIS will take place next week on **February 27 and March 1**.  \n",
    "- During the defense, I will ask you to explain your solutions, and you may also receive additional questions.  \n",
    "- If you are working in a group of two and are not from the same practical class, please contact me in advance to schedule your defense during office hours.  \n",
    "- Unjustified absences will result in a zero for the assignment, even if you have completed and submitted your work.  \n",
    "\n",
    "\n",
    "**THE CODE SHOULD BE RUN AND RESULTS NOT CLEARED**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 : Gauss-Markov Assumptions\n",
    "\n",
    "- **This exercise should be solved on PAPER (and photo or scan should be send).**\n",
    "\n",
    "- Proofs should be DETAILED (otherwise they aren't proofs...). \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Consider the multi-linear model:\n",
    "\n",
    "$\n",
    "y_i = X \\beta + \\epsilon_i\n",
    "$\n",
    "\n",
    "where:\n",
    "- $y_i$ is an $(n \\times 1)$ vector of observations,\n",
    "- $X$ is an $(n \\times k)$ matrix of independent variables,\n",
    "- $\\beta$ is a $(k \\times 1)$ vector of parameters,\n",
    "- $\\epsilon_i$ is an $(n \\times 1)$ vector of error terms.\n",
    "\n",
    "And the OLS estimator $\\hat{\\beta} = (X'X)^{-1} X'y$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "---\n",
    "#### 1. What does the Gauss-Markov theorem states ?\n",
    "The Gauss-Markov theorem states that, under the assumptions of the classical linear regression model, the Ordinary Least Squares (OLS) estimator β^\n",
    "is the Best Linear Unbiased Estimator (BLUE) of the coefficients β. \n",
    "This means that among all linear and unbiased estimators, β^ has the smallest variance.\n",
    "---\n",
    "#### 2. Which assumptions do you need in order to show that $\\hat{\\beta}$ is identified? Show identification.\n",
    "\n",
    "To show that β^ is identified, we need the following assumptions: Linearity: The relationship between the dependent variable yi and the independent variables X is linear.\n",
    "Full Rank: The matrix X has full column rank, i.e., X′X is invertible.\n",
    "Exogeneity: The error term ϵi has an expected value of zero given the independent variables, E[ϵi|X]=0.\n",
    "                            Identification\n",
    "Given the OLS estimator B^=(X′X)−1X′y, we can substitute y from the model y = Xβ+ϵ:\n",
    "            β^=(X′X)−1X′(Xβ+ϵ)\n",
    "This simplifies to:\n",
    "            β^=(X′X)−1X′Xβ+(X′X)−1X′ϵ\n",
    "            B^=β+(X′X)−1X′ϵ         \n",
    "For β^ to be identified, (X′X)−1X′ must exist, which is guaranteed if X′X is invertible (full rank assumption).\n",
    "\n",
    "---\n",
    "#### 3. Show that $\\hat{\\beta}$ is unbiased, \n",
    "\n",
    "i.e., $\\mathbb{E}[\\hat{\\beta}] = \\beta$.\n",
    "\n",
    "Starting from the expression derived above:\n",
    "    $\\hatβ=β+(X′X)−1X′ϵ$\n",
    "Taking the expectation:\n",
    "    E[β^]=E[β+(X′X)−1X′ϵ]\n",
    "Since β is a constant, and E[ϵ]=0:\n",
    "    E[β^]=β+(X′X)−1X′E[ϵ]\n",
    "    E[β^]=β+(X′X)−1X′⋅0\n",
    "    E[β^]=β\n",
    "Thus, \n",
    "β^ is unbiased.\n",
    "\n",
    "---\n",
    "#### 4. Prove that $\\operatorname{Var}(\\hat{\\beta}) = \\sigma^2 (X'X)^{-1}$.\n",
    "\n",
    "Using the expression β^=β+(X′X)−1X′ϵ, we can find the variance:\n",
    "\n",
    "$$\n",
    "\\operatorname{Var}(\\hat{\\beta}) = \\operatorname{Var}(\\beta + (X'X)^{-1} X'\\epsilon)\n",
    "$$\n",
    "\n",
    "Since β is a constant, its variance is zero, so:\n",
    "$$\n",
    "\\operatorname{Var}(\\hat{\\beta}) = \\operatorname{Var}((X'X)^{-1} X'\\epsilon)\n",
    "$$\n",
    "$\\operatorname{Var}(A\\epsilon) = A \\operatorname{Var}(\\epsilon) A'$\n",
    "\n",
    "Using the property, , where A=(X′X)−1X′:\n",
    "$$\n",
    "\\operatorname{Var}(\\hat{\\beta}) = (X'X)^{-1} X' \\operatorname{Var}(\\epsilon) X (X'X)^{-1}\n",
    "$$\n",
    "\n",
    "Assuming The following macros are not allowed: operatorname\n",
    "$\\operatorname{Var}(\\epsilon) = \\sigma^2 I_n$, \n",
    "where In is the identity matrix of size n:\n",
    "$$\n",
    "\\operatorname{Var}(\\hat{\\beta}) = (X'X)^{-1} X' \\sigma^2 I_n X (X'X)^{-1}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\operatorname{Var}(\\hat{\\beta}) = \\sigma^2 (X'X)^{-1} X' X (X'X)^{-1}\n",
    "$$\n",
    "\n",
    "Since X′X is symmetric and invertible, and X′X itself:\n",
    "$$\n",
    "\\operatorname{Var}(\\hat{\\beta}) = \\sigma^2 (X'X)^{-1}\n",
    "$$\n",
    "\n",
    "\n",
    "---\n",
    "#### 5. Efficiency of $\\hat{\\beta}$ and the Gauss-Markov Theorem  \n",
    "\n",
    "Let $\\tilde{\\beta}$ be another **linear and unbiased** estimator of $\\beta$. That is,  \n",
    "\n",
    "\n",
    "$\\tilde{\\beta} = Ay$\n",
    "\n",
    "\n",
    "for some matrix $A$ such that $\\mathbb{E}[\\tilde{\\beta}] = \\beta$.  \n",
    "\n",
    "Using **Cochran’s theorem**, show that the variance of any other linear unbiased estimator $\\tilde{\\beta}$ is always at least as large as the variance of $\\hat{\\beta}$, the OLS estimator.  \n",
    "\n",
    "Conclude that $\\hat{\\beta}$ is the **Best Linear Unbiased Estimator (BLUE)** according to the **Gauss-Markov theorem**.  \n",
    "\n",
    "---\n",
    "#### 6. Consistency of $\\hat{\\beta}$  \n",
    "\n",
    "A sequence of estimators $\\hat{\\beta}_n$ is said to be **consistent** for $\\beta$ if:  \n",
    "\n",
    "$\\hat{\\beta}_n \\xrightarrow{p} \\beta \\quad \\text{(convergence in probability)}.$\n",
    "\n",
    "This means that as the sample size increases, $\\hat{\\beta}_n$ gets arbitrarily close to $\\beta$ with high probability.  \n",
    "\n",
    "Using the **Law of Large Numbers (LLN)**, prove that the OLS estimator $\\hat{\\beta}$ is consistent under standard assumptions.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Exercise 2 : Linear Dependence and Rank of a Matrix\n",
    "\n",
    "**Understanding Linear Dependence in Matrices**\n",
    "\n",
    "In the context of a matrix, **linear dependence** refers to a situation where one or more columns (or rows) can be written as a linear combination of other columns (or rows). If all columns (or rows) are linearly independent, no such relationship exists.\n",
    "\n",
    "A set of column vectors $ \\mathbf{v}_1, \\mathbf{v}_2, \\dots, \\mathbf{v}_n $ in $ \\mathbb{R}^m $ is **linearly dependent** if there exist scalars $ c_1, c_2, \\dots, c_n $, **not all zero**, such that:\n",
    "\n",
    "$$\n",
    "c_1 \\mathbf{v}_1 + c_2 \\mathbf{v}_2 + \\dots + c_n \\mathbf{v}_n = \\mathbf{0}\n",
    "$$\n",
    "\n",
    "Equivalently, a matrix $ A $ with columns $ \\mathbf{v}_1, \\mathbf{v}_2, \\dots, \\mathbf{v}_n $ is **linearly dependent** if there is a nontrivial solution to:\n",
    "\n",
    "$$\n",
    "A \\mathbf{c} = \\mathbf{0}\n",
    "$$\n",
    "\n",
    "where $ A = [\\mathbf{v}_1 \\, \\mathbf{v}_2 \\, \\dots \\, \\mathbf{v}_n] $ and $ \\mathbf{c} \\neq \\mathbf{0} $.\n",
    "\n",
    "One way to determine if a set of vectors (columns or rows) is linearly dependent is by checking the **rank** of the matrix.\n",
    "\n",
    "- The **rank** of a matrix $ A $ is the number of **linearly independent** columns (or rows).\n",
    "- If $ \\text{rank}(A) = n $ (the number of columns), the columns are **linearly independent**.\n",
    "- If $ \\text{rank}(A) < n $, then at least one column can be written as a linear combination of the others, meaning the columns are **linearly dependent**.\n",
    "\n",
    "To compute the rank of a matrix, we can transform it into **Row Echelon Form (REF)** using **Gaussian Elimination**. The number of nonzero rows in REF corresponds to the rank:\n",
    "\n",
    "$$\n",
    "\\text{rank}(A) = \\text{number of nonzero rows in REF}\n",
    "$$\n",
    "\n",
    "If $ \\text{rank}(A) < n $, the matrix has **linearly dependent columns**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### **Task: Implement Rank Calculation Using Gaussian Elimination**\n",
    "Your goal is to implement a function that determines the rank of a matrix using Gaussian Elimination, without relying on external libraries (like NumPy or SciPy). \n",
    "Then, using your function, determine whether the given matrices are linearly independent.\n",
    "\n",
    "### **Instructions**\n",
    "1. Implement the function `rank_of_matrix(matrix: list[list[float]]) -> int` that:\n",
    "   - Takes a matrix as a list of lists.\n",
    "   - Performs Gaussian elimination to convert it into row echelon form.\n",
    "   - Returns the rank (i.e., the number of nonzero rows in the echelon form).\n",
    "\n",
    "2. Use your function to determine if the following matrices have **linearly independent columns**.\n",
    "\n",
    "### **Example Matrices**\n",
    "```python\n",
    "matrix1 = [\n",
    "    [1, 2, 3],\n",
    "    [4, 5, 6],\n",
    "    [7, 8, 9]\n",
    "]\n",
    "\n",
    "matrix2 = [\n",
    "    [1, 0, 0],\n",
    "    [0, 1, 0],\n",
    "    [0, 0, 1]\n",
    "]\n",
    "\n",
    "matrix3 = [\n",
    "    [1, 2, 3, 4],\n",
    "    [4, 5, 6, 7],\n",
    "    [7, 8, 9, 10],\n",
    "    [1, 1, 1, 1]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rank' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 9\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m rank\n\u001b[0;32m      6\u001b[0m matrix1 \u001b[38;5;241m=\u001b[39m [[\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m], \n\u001b[0;32m      7\u001b[0m            [\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m6\u001b[39m], \n\u001b[0;32m      8\u001b[0m            [\u001b[38;5;241m7\u001b[39m, \u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m10\u001b[39m]]\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRank of matrix1:\u001b[39m\u001b[38;5;124m\"\u001b[39m, rank_matrix(matrix1))\n\u001b[0;32m     11\u001b[0m matrix2 \u001b[38;5;241m=\u001b[39m [[\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m], \n\u001b[0;32m     12\u001b[0m            [\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m6\u001b[39m], \n\u001b[0;32m     13\u001b[0m            [\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m6\u001b[39m, \u001b[38;5;241m9\u001b[39m]]\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRank of matrix2:\u001b[39m\u001b[38;5;124m\"\u001b[39m, rank_matrix(matrix2))\n",
      "Cell \u001b[1;32mIn[2], line 4\u001b[0m, in \u001b[0;36mrank_matrix\u001b[1;34m(matrix)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns the rank of a matrix using Gaussian elimination.\"\"\"\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# YOUR CODE GOES HERE\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m rank\n",
      "\u001b[1;31mNameError\u001b[0m: name 'rank' is not defined"
     ]
    }
   ],
   "source": [
    "def rank_matrix(matrix):\n",
    "    \"\"\"Returns the rank of a matrix using Gaussian elimination.\"\"\"\n",
    "    # YOUR CODE GOES HERE\n",
    "    return rank\n",
    "\n",
    "matrix1 = [[1, 2, 3], \n",
    "           [4, 5, 6], \n",
    "           [7, 8, 10]]\n",
    "print(\"Rank of matrix1:\", rank_matrix(matrix1))\n",
    "\n",
    "matrix2 = [[1, 2, 3], \n",
    "           [2, 4, 6], \n",
    "           [3, 6, 9]]\n",
    "print(\"Rank of matrix2:\", rank_matrix(matrix2))\n",
    "\n",
    "matrix3 = [\n",
    "    [1, 2, 3, 4],\n",
    "    [4, 5, 6, 7],\n",
    "    [7, 8, 9, 10],\n",
    "    [1, 1, 1, 1]\n",
    "]\n",
    "print(\"Rank of matrix3:\", rank_matrix(matrix3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rank_matrix(matrix):\n",
    "    \"\"\"Returns the rank of a matrix using Gaussian elimination.\"\"\"\n",
    "    if not matrix:\n",
    "        return 0\n",
    "    m, n = len(matrix), len(matrix[0])\n",
    "    rank = 0\n",
    "    for r in range(n):\n",
    "        if matrix[rank][r] != 0:\n",
    "            for c in range(rank + 1, m):\n",
    "                ratio = matrix[c][r] / matrix[rank][r]\n",
    "                for k in range(n):\n",
    "                    matrix[c][k] -= ratio * matrix[rank][k]\n",
    "            rank += 1\n",
    "        else:\n",
    "            for i in range(rank + 1, m):\n",
    "                if matrix[i][r] != 0:\n",
    "                    matrix[rank], matrix[i] = matrix[i], matrix[rank]\n",
    "                    rank -= 1\n",
    "                    break\n",
    "    return rank\n",
    "\n",
    "matrix1 = [[1, 2, 3], \n",
    "           [4, 5, 6], \n",
    "           [7, 8, 10]]\n",
    "print(\"Rank of matrix1:\", rank_matrix(matrix1))\n",
    "\n",
    "matrix2 = [[1, 2, 3], \n",
    "           [2, 4, 6], \n",
    "           [3, 6, 9]]\n",
    "print(\"Rank of matrix2:\", rank_matrix(matrix2))\n",
    "\n",
    "matrix3 = [\n",
    "    [1, 2, 3, 4],\n",
    "    [4, 5, 6, 7],\n",
    "    [7, 8, 9, 10],\n",
    "    [1, 1, 1, 1]\n",
    "]\n",
    "print(\"Rank of matrix3:\", rank_matrix(matrix3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3 : Coding tasks\n",
    "\n",
    "Consider the following dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "X = np.linspace(-3, 3, 1_000).reshape(-1, 1)\n",
    "y = 2 * X**2 + 3 * X + 5 + np.random.normal(0, 2, X.shape)\n",
    "\n",
    "plt.scatter(X, y, color='blue', label='Data points')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.title('Non-Linear Dataset')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step 1**\n",
    "\n",
    "You should implement a custom regression class using the template below that does the following:\n",
    "- Parameters: Accepts a flag to include a constant (intercept) or not.\n",
    "- Fit: Estimate the OLS parameters using the formulas\n",
    "$$\\widehat a = \\frac{\\sum\\limits_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sum\\limits_{i=1}^n (x_i - \\bar{x})^2}, \\quad \n",
    "    \\widehat b = \\bar{y} - \\widehat a \\bar{x}.$$\n",
    "- Predict: Compute predictions using the estimated parameters.\n",
    "- Residuals: Return the residuals, defined as:\n",
    "$$ residuals = y − \\widehat y $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CustomLinearRegression:\n",
    "    def __init__(self, use_intercept=True):\n",
    "        self.use_intercept = use_intercept\n",
    "        self.coef_ = None\n",
    "        self.intercept_ = 0\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        # YOUR CODE GOES HERE\n",
    "            pass\n",
    "    def predict(self, X):\n",
    "        # YOUR CODE GOES HERE\n",
    "            pass\n",
    "    def residuals(self, X, y):\n",
    "        # YOUR CODE GOES HERE\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CustomLinearRegression:\n",
    "    def __init__(self, use_intercept=True):\n",
    "        self.use_intercept = use_intercept\n",
    "        self.coef_ = None\n",
    "        self.intercept_ = 0\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        if self.use_intercept:\n",
    "            X = np.c_[np.ones(X.shape[0]), X]\n",
    "        \n",
    "        # Closed-form solution (Normal Equation)\n",
    "        X_transpose = X.T\n",
    "        self.coef_ = np.linalg.inv(X_transpose.dot(X)).dot(X_transpose).dot(y)\n",
    "        \n",
    "        if self.use_intercept:\n",
    "            self.intercept_ = self.coef_[0]\n",
    "            self.coef_ = self.coef_[1:]\n",
    "    \n",
    "    def predict(self, X):\n",
    "        if self.use_intercept:\n",
    "            return self.intercept_ + np.dot(X, self.coef_)\n",
    "        else:\n",
    "            return np.dot(X, self.coef_)\n",
    "    \n",
    "    def residuals(self, X, y):\n",
    "        return y - self.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2\n",
    "- Split the dataset into training and testing sets\n",
    "- Use your custom linear regression class to fit the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CustomLinearRegression:\n",
    "    def __init__(self, use_intercept=True):\n",
    "        self.use_intercept = use_intercept\n",
    "        self.coef_ = None\n",
    "        self.intercept_ = 0\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        if self.use_intercept:\n",
    "            X = np.c_[np.ones(X.shape[0]), X]\n",
    "        \n",
    "        # Closed-form solution (Normal Equation)\n",
    "        X_transpose = X.T\n",
    "        self.coef_ = np.linalg.inv(X_transpose.dot(X)).dot(X_transpose).dot(y)\n",
    "        \n",
    "        if self.use_intercept:\n",
    "            self.intercept_ = self.coef_[0]\n",
    "            self.coef_ = self.coef_[1:]\n",
    "    \n",
    "    def predict(self, X):\n",
    "        if self.use_intercept:\n",
    "            return self.intercept_ + np.dot(X, self.coef_)\n",
    "        else:\n",
    "            return np.dot(X, self.coef_)\n",
    "    \n",
    "    def residuals(self, X, y):\n",
    "        return y - self.predict(X)\n",
    "# YOUR CODE GOES HERE\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create and fit the model\n",
    "model = CustomLinearRegression()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3\n",
    "- Compute predictions for the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# YOUR CODE GOES HERE\n",
    "# Compute predictions\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4\n",
    "\n",
    "- Plot the actual data points and overlay the regression line from your custom model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# YOUR CODE GOES HERE\n",
    "# Plot the actual data points\n",
    "plt.scatter(X_test, y_test, color='blue', label='Actual data points')\n",
    "# Plot the regression line\n",
    "plt.plot(X_test, y_pred, color='red', label='Regression line')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.title('Actual Data Points and Regression Line')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5\n",
    "- Create a plot of residuals \n",
    "- Are the residuals randomly dispersed around zero?\n",
    "- What does the pattern (or lack thereof) tell you about the appropriateness of a linear model for this dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# YOUR CODE GOES HERE\n",
    "# Compute residuals\n",
    "residuals = model.residuals(X_test, y_test)\n",
    "\n",
    "# Plot residuals\n",
    "plt.scatter(X_test, residuals, color='purple', label='Residuals')\n",
    "plt.axhline(y=0, color='black', linestyle='--')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residuals Plot')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6\n",
    "\n",
    "- Use sklearn to implement Polynomial Regression and fit it on the same dataset.\n",
    "- Compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# YOUR CODE GOES HERE\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Transform the data to include polynomial terms\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "X_poly_train = poly.fit_transform(X_train)\n",
    "X_poly_test = poly.transform(X_test)\n",
    "\n",
    "# Fit the polynomial regression model\n",
    "poly_model = LinearRegression()\n",
    "poly_model.fit(X_poly_train, y_train)\n",
    "\n",
    "# Predict using the polynomial model\n",
    "y_poly_pred = poly_model.predict(X_poly_test)\n",
    "\n",
    "# Plot the actual data points and the polynomial regression line\n",
    "plt.scatter(X_test, y_test, color='blue', label='Actual data points')\n",
    "plt.scatter(X_test, y_poly_pred, color='red', label='Polynomial regression line')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.title('Polynomial Regression')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Discussion\n",
    "\n",
    "- Discuss how the linear model may underfit the data because it cannot capture the inherent non-linear relationships\n",
    "- Explain how introducing polynomial terms leads to a better fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Discuss how the linear model may underfit the data because it cannot capture the inherent non-linear relationships\n",
    "# Explain how introducing polynomial terms leads to a better fit\n",
    "\n",
    "# In this exercise, we implemented both a custom linear regression model and a polynomial regression model to fit a given dataset. \n",
    "# The dataset was generated to have a quadratic relationship, and the linear model struggled to capture this non-linear relationship.\n",
    "\n",
    "# Linear Model Analysis:\n",
    "# The linear regression model attempts to fit a straight line through the data points. \n",
    "# However, since the true relationship between the dependent variable (y) and the independent variable (X) is quadratic, \n",
    "# the linear model underfits the data. This underfitting is evident from the residuals plot, where the residuals exhibit a clear pattern \n",
    "# rather than being randomly dispersed around zero. Such a pattern indicates that the linear model fails to capture the true underlying relationship, \n",
    "# leading to biased and inefficient estimates.\n",
    "\n",
    "# Polynomial Model Analysis:\n",
    "# By introducing polynomial terms, we transformed the dataset to include quadratic relationships. \n",
    "# The polynomial regression model was then able to fit a curve that closely follows the true quadratic relationship between X and y. \n",
    "# This improved fit is evident from the polynomial regression line closely matching the actual data points. \n",
    "# Additionally, the residuals plot for the polynomial model would show residuals randomly dispersed around zero, indicating a better fit.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
