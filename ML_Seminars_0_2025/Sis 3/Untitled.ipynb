{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8739e56d-c8b7-4091-b2fd-5350839f9307",
   "metadata": {},
   "source": [
    "# Logistic Regression for Identifying Depression Among Students\n",
    "\n",
    "## Overview\n",
    "In this task, you will use Logistic Regression to predict whether a student is experiencing depression based on different factors such as academic pressure, sleep habits, and financial stress.\n",
    "\n",
    "## Dataset\n",
    "We will use the Student Depression Dataset (good timing after mid term exam I guess).\n",
    "\n",
    "### Features:\n",
    "- **Gender**: Male/Female\n",
    "- **Age**: Student’s age\n",
    "- **City**: City where the student lives\n",
    "- **Academic Pressure**: Level of academic stress\n",
    "- **Work Pressure**: Level of work-related stress\n",
    "- **CGPA**: Student’s academic performance\n",
    "- **Study Satisfaction**: How satisfied the student is with their studies\n",
    "- **Job Satisfaction**: Satisfaction with a job (if applicable)\n",
    "- **Sleep Duration**: Sleep hours category (e.g., \"Less than 5 hours\", \"5-6 hours\")\n",
    "- **Dietary Habits**: Eating habits (e.g., Healthy, Moderate)\n",
    "- **Degree**: The degree the student is pursuing\n",
    "- **Suicidal Thoughts**: Whether the student has had suicidal thoughts (Yes/No)\n",
    "- **Work/Study Hours**: Hours spent working or studying daily\n",
    "- **Financial Stress**: Level of financial pressure\n",
    "- **Family History of Mental Illness**: Whether the student has a family history of mental illness (Yes/No)\n",
    "- **Depression**: (Target variable: 1 = Has depression, 0 = No depression)\n",
    "\n",
    "## Tasks\n",
    "### **Use the df DataFrame from the cell below for all tasks.**\n",
    "### Task 1 – Explore the Data\n",
    "Understand the dataset and find interesting patterns :\n",
    "- Use basic pandas functions to check the data.\n",
    "- Look for missing values, outliers, and patterns in the features.\n",
    "- Create visualizations such as heatmap, histograms, bar charts, scatter plots etc. to explore relationships between different features and depression.\n",
    "\n",
    "### Task 2 – Data Preprocessing\n",
    "Clean and prepare the data for modeling : \n",
    "- Handle missing values (e.g., filling or removing them). \n",
    "- Convert categorical variables into numbers using encoding. \n",
    "- Normalize or standardize numerical features if needed.\n",
    "- Explain why you made certain preprocessing choices.\n",
    "\n",
    "### Task 3 – Train the Model\n",
    "Train a Logistic Regression model to predict depression : \n",
    "- Split the data into training and testing sets.\n",
    "- Train a Logistic Regression model using scikit-learn. Look at the different parameters in scikit learn libraries ant try to change some of them (only if you can understand them).\n",
    "- Make predictions on the test data.\n",
    "\n",
    "### Task 4 – Evaluate the Model\n",
    "Measure how well the model performs : \n",
    "- Calculate accuracy, precision, recall, and F1-score.\n",
    "- Create a confusion matrix to see how often the model makes correct and incorrect predictions.\n",
    "- Plot an ROC curve to analyze model performance.\n",
    "- Think on how we could analyse predicted probabilities\n",
    "\n",
    "#### Some documentation \n",
    "\n",
    "[Seaborn Heatmap](https://seaborn.pydata.org/generated/seaborn.heatmap.html)\n",
    "\n",
    "[Pairplot](https://seaborn.pydata.org/generated/seaborn.pairplot.html)\n",
    "(There are many different plots and possible analysis to do with Seaborn. Navigate by yourself and feel free to do some insighful analysis)\n",
    "\n",
    "[Imputing values](https://scikit-learn.org/stable/modules/impute.html)\n",
    "\n",
    "[scikit-learn preprocessing documentation](https://scikit-learn.org/stable/modules/preprocessing.html)\n",
    "\n",
    "[Logistic regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)\n",
    "\n",
    "[Classification metrics](https://scikit-learn.org/stable/modules/model_evaluation.html#classification-metrics)\n",
    "\n",
    "---\n",
    "---\n",
    "\n",
    "## Part two\n",
    "\n",
    "### Task 1 – Evaluate and Visualize Metric Changes Across Thresholds\n",
    "As we have seen in class, different metrics varry according to the threshold. Explain why.\n",
    "\n",
    "Compute the TN, TP, FN and FP manually. \n",
    "Find a way to visualise their evolution according to the threshold. \n",
    "Explain the different variations. What does it tell about your predictions ?\n",
    "\n",
    "Do the same with evaluation metrics.\n",
    "\n",
    "How would you define the optimal threshold? \n",
    "\n",
    "### Task 2 - Include a financial cost\n",
    "In the context of predicting student depression, if the model outputs a positive prediction (the student is predicted as being depressed), you must pay for the student to go to the hospital. [...]\n",
    "This cost is substantial, therefore, it is crucial not only to evaluate the standard classification metrics—such as accuracy, precision, recall, and F1-score—but also to consider the fina[...]\n",
    "\n",
    "Find a way to define the optimal threshold, including the cost of a positive prediction.\n",
    "\n",
    "### Task 3 - Cross validation & hyperparameters optimization\n",
    "What are the hyperparameters in the logistic regression? \n",
    "Change your code to find the optimal hyperparameters and train it with cross validation\n",
    "\n",
    "### Task 4 - Investigate and understand predictions\n",
    "Now that you have an optimized classification (almost), you want to understand why it predicts some students as depressed.\n",
    "You might be interested by analysing the difference of feature values for different predicted populations (TP TN FP & FN).\n",
    "You could also have a look at the weights of you logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8502945d-24f4-45d4-9b26-bbe6dbe857f9",
   "metadata": {},
   "source": [
    "# Logistic Regression for Identifying Depression Among Students\n",
    "\n",
    "## Overview\n",
    "In this task, you will use Logistic Regression to predict whether a student is experiencing depression based on different factors such as academic pressure, sleep habits, and financial stress.\n",
    "\n",
    "## Dataset\n",
    "We will use the Student Depression Dataset.\n",
    "\n",
    "### Features:\n",
    "- **Gender**: Male/Female\n",
    "- **Age**: Student’s age\n",
    "- **City**: City where the student lives\n",
    "- **Academic Pressure**: Level of academic stress\n",
    "- **Work Pressure**: Level of work-related stress\n",
    "- **CGPA**: Student’s academic performance\n",
    "- **Study Satisfaction**: How satisfied the student is with their studies\n",
    "- **Job Satisfaction**: Satisfaction with a job (if applicable)\n",
    "- **Sleep Duration**: Sleep hours category (e.g., \"Less than 5 hours\", \"5-6 hours\")\n",
    "- **Dietary Habits**: Eating habits (e.g., Healthy, Moderate)\n",
    "- **Degree**: The degree the student is pursuing\n",
    "- **Suicidal Thoughts**: Whether the student has had suicidal thoughts (Yes/No)\n",
    "- **Work/Study Hours**: Hours spent working or studying daily\n",
    "- **Financial Stress**: Level of financial pressure\n",
    "- **Family History of Mental Illness**: Whether the student has a family history of mental illness (Yes/No)\n",
    "- **Depression**: (Target variable: 1 = Has depression, 0 = No depression)\n",
    "\n",
    "## Tasks\n",
    "\n",
    "### Task 1 – Explore the Data\n",
    "Understand the dataset and find interesting patterns:\n",
    "- Use basic pandas functions to check the data.\n",
    "- Look for missing values, outliers, and patterns in the features.\n",
    "- Create visualizations such as heatmap, histograms, bar charts, scatter plots etc. to explore relationships between different features and depression.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('student_depression.csv')\n",
    "\n",
    "# Basic data exploration\n",
    "print(df.head())\n",
    "print(df.info())\n",
    "print(df.describe())\n",
    "\n",
    "# Check for missing values\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Visualizations\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(df.corr(), annot=True, cmap='coolwarm')\n",
    "plt.title('Heatmap of Feature Correlations')\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(df['Age'], kde=True)\n",
    "plt.title('Distribution of Age')\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Gender', y='Depression', data=df)\n",
    "plt.title('Depression by Gender')\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x='CGPA', y='Depression', data=df)\n",
    "plt.title('Depression by CGPA')\n",
    "\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### Task 2 – Data Preprocessing\n",
    "Clean and prepare the data for modeling:\n",
    "- Handle missing values (e.g., filling or removing them).\n",
    "- Convert categorical variables into numbers using encoding.\n",
    "- Normalize or standardize numerical features if needed.\n",
    "- Explain why you made certain preprocessing choices.\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "# Handle missing values\n",
    "df.fillna(df.mean(), inplace=True)\n",
    "\n",
    "# Encode categorical variables\n",
    "label_encoders = {}\n",
    "categorical_columns = ['Gender', 'City', 'Dietary Habits', 'Degree', 'Suicidal Thoughts', 'Family History of Mental Illness']\n",
    "\n",
    "for column in categorical_columns:\n",
    "    le = LabelEncoder()\n",
    "    df[column] = le.fit_transform(df[column])\n",
    "    label_encoders[column] = le\n",
    "\n",
    "# Standardize numerical features\n",
    "scaler = StandardScaler()\n",
    "numerical_columns = ['Age', 'Academic Pressure', 'Work Pressure', 'CGPA', 'Study Satisfaction', 'Job Satisfaction', 'Sleep Duration', 'Work/Study Hours', 'Financial Stress']\n",
    "\n",
    "df[numerical_columns] = scaler.fit_transform(df[numerical_columns])\n",
    "\n",
    "# Explanation of preprocessing choices:\n",
    "# - Filled missing values with mean to maintain the dataset size.\n",
    "# - Encoded categorical variables to numerical values for model compatibility.\n",
    "# - Standardized numerical features to ensure they have a mean of 0 and a standard deviation of 1, which helps the model converge faster.\n",
    "```\n",
    "\n",
    "### Task 3 – Train the Model\n",
    "Train a Logistic Regression model to predict depression:\n",
    "- Split the data into training and testing sets.\n",
    "- Train a Logistic Regression model using scikit-learn. Look at the different parameters in scikit learn libraries and try to change some of them (only if you can understand them).\n",
    "- Make predictions on the test data.\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_curve, auc\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X = df.drop('Depression', axis=1)\n",
    "y = df['Depression']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train the Logistic Regression model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_prob = model.predict_proba(X_test)[:, 1]\n",
    "```\n",
    "\n",
    "### Task 4 – Evaluate the Model\n",
    "Measure how well the model performs:\n",
    "- Calculate accuracy, precision, recall, and F1-score.\n",
    "- Create a confusion matrix to see how often the model makes correct and incorrect predictions.\n",
    "- Plot an ROC curve to analyze model performance.\n",
    "- Think on how we could analyze predicted probabilities.\n",
    "\n",
    "```python\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')\n",
    "print(f'F1 Score: {f1}')\n",
    "print(f'Confusion Matrix:\\n {conf_matrix}')\n",
    "\n",
    "# Plot ROC curve\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "# Analyze predicted probabilities\n",
    "print(y_pred_prob)\n",
    "```\n",
    "\n",
    "## Part two\n",
    "\n",
    "### Task 1 – Evaluate and Visualize Metric Changes Across Thresholds\n",
    "As we have seen in class, different metrics vary according to the threshold. Explain why.\n",
    "\n",
    "Compute the TN, TP, FN and FP manually. \n",
    "Find a way to visualize their evolution according to the threshold. \n",
    "Explain the different variations. What does it tell about your predictions?\n",
    "\n",
    "Do the same with evaluation metrics.\n",
    "\n",
    "How would you define the optimal threshold?\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "thresholds = np.arange(0.0, 1.0, 0.05)\n",
    "metrics = []\n",
    "\n",
    "for threshold in thresholds:\n",
    "    y_pred_threshold = (y_pred_prob >= threshold).astype(int)\n",
    "    \n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred_threshold).ravel()\n",
    "    accuracy = accuracy_score(y_test, y_pred_threshold)\n",
    "    precision = precision_score(y_test, y_pred_threshold)\n",
    "    recall = recall_score(y_test, y_pred_threshold)\n",
    "    f1 = f1_score(y_test, y_pred_threshold)\n",
    "    \n",
    "    metrics.append([threshold, tn, fp, fn, tp, accuracy, precision, recall, f1])\n",
    "\n",
    "metrics = np.array(metrics)\n",
    "\n",
    "# Plot metrics\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(metrics[:, 0], metrics[:, 1], label='True Negative')\n",
    "plt.plot(metrics[:, 0], metrics[:, 2], label='False Positive')\n",
    "plt.plot(metrics[:, 0], metrics[:, 3], label='False Negative')\n",
    "plt.plot(metrics[:, 0], metrics[:, 4], label='True Positive')\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Confusion Matrix Components vs. Threshold')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(metrics[:, 0], metrics[:, 5], label='Accuracy')\n",
    "plt.plot(metrics[:, 0], metrics[:, 6], label='Precision')\n",
    "plt.plot(metrics[:, 0], metrics[:, 7], label='Recall')\n",
    "plt.plot(metrics[:, 0], metrics[:, 8], label='F1 Score')\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Metric')\n",
    "plt.title('Evaluation Metrics vs. Threshold')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Explanation of variations:\n",
    "# - As the threshold increases, the number of true positives and false positives generally decreases, while the number of true negatives and false negatives increases.\n",
    "# - The optimal threshold can be defined based on the specific requirements of the problem, such as maximizing precision, recall, or a combination of both (F1 score).\n",
    "```\n",
    "\n",
    "### Task 2 - Include a financial cost\n",
    "In the context of predicting student depression, if the model outputs a positive prediction (the student is predicted as being depressed), you must pay for the student to go to the hospital. This cost is substantial, therefore, it is crucial not only to evaluate the standard classification metrics—such as accuracy, precision, recall, and F1-score—but also to consider the financial cost.\n",
    "\n",
    "Find a way to define the optimal threshold, including the cost of a positive prediction.\n",
    "\n",
    "```python\n",
    "cost_per_prediction = 1000  # Example cost per positive prediction\n",
    "total_costs = []\n",
    "\n",
    "for threshold in thresholds:\n",
    "    y_pred_threshold = (y_pred_prob >= threshold).astype(int)\n",
    "    fp_cost = fp * cost_per_prediction\n",
    "    total_cost = fp_cost  # Add other costs if applicable\n",
    "    total_costs.append(total_cost)\n",
    "\n",
    "total_costs = np.array(total_costs)\n",
    "\n",
    "# Plot total costs vs. threshold\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(thresholds, total_costs, label='Total Cost')\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Total Cost')\n",
    "plt.title('Total Cost vs. Threshold')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Optimal threshold can be defined as the one that minimizes the total cost\n",
    "optimal_threshold = thresholds[np.argmin(total_costs)]\n",
    "print(f'Optimal Threshold: {optimal_threshold}')\n",
    "```\n",
    "\n",
    "### Task 3 - Cross validation & hyperparameters optimization\n",
    "What are the hyperparameters in the logistic regression? \n",
    "Change your code to find the optimal hyperparameters and train it with cross validation.\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Hyperparameters for logistic regression\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'solver': ['liblinear', 'saga'],\n",
    "    'max_iter': [100, 200, 300]\n",
    "}\n",
    "\n",
    "# Grid search with cross-validation\n",
    "grid_search = GridSearchCV(LogisticRegression(), param_grid, cv=5, scoring='f1')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "print(f'Best Hyperparameters: {best_params}')\n",
    "\n",
    "# Train with best hyperparameters\n",
    "model_optimized = LogisticRegression(**best_params)\n",
    "model_optimized.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions with the optimized model\n",
    "y_pred_optimized = model_optimized.predict(X_test)\n",
    "y_pred_prob_optimized = model_optimized.predict_proba(X_test)[:, 1]\n",
    "```\n",
    "\n",
    "### Task 4 - Investigate and understand predictions\n",
    "Now that you have an optimized classification (almost), you want to understand why it predicts some students as depressed.\n",
    "You might be interested by analyzing the difference of feature values for different predicted populations (TP, TN, FP & FN).\n",
    "You could also have a look at the weights of your logistic regression.\n",
    "\n",
    "```python\n",
    "# Analyzing feature values for different predicted populations\n",
    "df_test = X_test.copy()\n",
    "df_test['True Label'] = y_test\n",
    "df_test['Predicted Label'] = y_pred_optimized\n",
    "\n",
    "true_positives = df_test[(df_test['True Label'] == 1) & (df_test['Predicted Label'] == 1)]\n",
    "true_negatives = df_test[(df_test['True Label'] == 0) & (df_test['Predicted Label'] == 0)]\n",
    "false_positives = df_test[(df_test['True Label'] == 0) & (df_test['Predicted Label'] == 1)]\n",
    "false_negatives = df_test[(df_test['True Label'] == 1) & (df_test['Predicted Label'] == 0)]\n",
    "\n",
    "print('True Positives:')\n",
    "print(true_positives.describe())\n",
    "print('True Negatives:')\n",
    "print(true_negatives.describe())\n",
    "print('False Positives:')\n",
    "print(false_positives.describe())\n",
    "print('False Negatives:')\n",
    "print(false_negatives.describe())\n",
    "\n",
    "# Weights of logistic regression\n",
    "feature_weights = pd.DataFrame({'Feature': X.columns, 'Weight': model_optimized.coef_[0]})\n",
    "feature_weights = feature_weights.sort_values(by='Weight', ascending=False)\n",
    "print(feature_weights)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a80a2c-1435-4b3a-8bad-af1aa5994804",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c54780b2-d3ed-4612-8bbc-f5e2549e9ad4",
   "metadata": {},
   "source": [
    "# **K-Nearest Neighbors**\n",
    "\n",
    "In this exercise, you will build a classifier using the K-Nearest Neighbors algorithm.\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Generating data**\n",
    "\n",
    "You will start by generating synthetic data for this problem.  \n",
    "\n",
    "- Generate a dataset with **n = 50 points** in a unit square. Each point should have an assigned label: orange or blue.  \n",
    "  - Think about the rules to assign the labels (hint: you need a boundary rule to classify points as orange or blue).  \n",
    "  - Store the x and y coordinates, along with the label, in a structured format.  \n",
    "  - Visualize the dataset.\n",
    "\n",
    "**Solution:**\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate synthetic data\n",
    "n = 50\n",
    "np.random.seed(0)\n",
    "X = np.random.rand(n, 2)\n",
    "y = np.where(X[:, 0] + X[:, 1] > 1, 'orange', 'blue')\n",
    "\n",
    "# Visualize the dataset\n",
    "plt.scatter(X[y == 'orange'][:, 0], X[y == 'orange'][:, 1], color='orange', label='Orange')\n",
    "plt.scatter(X[y == 'blue'][:, 0], X[y == 'blue'][:, 1], color='blue', label='Blue')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.title('Generated Dataset')\n",
    "plt.show()```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed0fda7-1626-4a89-ad05-09b77ad6ee5d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
