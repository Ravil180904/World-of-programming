{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Central Limit Theorem (CLT)\n",
    "\n",
    "The **Central Limit Theorem** explains the statistical properties of the sample mean ($\\bar{X}_n$) when $n$ independent and identically distributed (i.i.d.) random variables ($X_1, X_2, \\dots, X_n$) are drawn from a population with a finite mean ($\\mu$) and finite variance ($\\sigma^2$).\n",
    "\n",
    "\n",
    "For a population where:\n",
    "- The mean is $\\mathbb{E}[X_n] = \\mu$, and\n",
    "- The variance is $\\text{Var}(X_n) = \\sigma^2$,\n",
    "\n",
    "The sample mean $\\bar{X}_n$ is defined as:\n",
    "\n",
    "$$\n",
    "\\bar{X}_n = \\frac{1}{n} \\sum_{k=1}^n X_k\n",
    "$$\n",
    "\n",
    "##### Properties of $\\bar{X}_n$:\n",
    "1. Expected Value ($\\mathbb{E}[\\bar{X}_n]$):\n",
    "   $$\\mathbb{E}[\\bar{X}_n] = \\mu$$\n",
    "   The expected value of the sample mean is equal to the population mean.\n",
    "\n",
    "2. Variance ($\\text{Var}(\\bar{X}_n)$):\n",
    "   $$\n",
    "   \\text{Var}(\\bar{X}_n) = \\frac{\\sigma^2}{n}\n",
    "   $$\n",
    "   The variance of the sample mean decreases as the sample size $n$ increases. Larger samples provide more precise estimates.\n",
    "\n",
    "\n",
    "---\n",
    "#### **CLT statement**: \n",
    "For large values of $n$, the sample mean $\\bar{X}_n$ is approximately **normally distributed**, regardless of the original population's distribution :\n",
    "\n",
    "$$\n",
    "\\bar{X}_n \\sim \\mathcal{N}\\left(\\mu, \\frac{\\sigma^2}{n}\\right) \\quad \\text{as } n \\to \\infty\n",
    "$$\n",
    "\n",
    "This means that $\\bar{X}_n$ follows a **normal distribution** with:\n",
    "- Mean: $\\mu$ (the population mean)\n",
    "- Variance: $\\frac{\\sigma^2}{n}$ (the population variance divided by the sample size).\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Intuition\n",
    "\n",
    "1. Population Shape Doesn't Matter:\n",
    "   - Even if the original population $X_k$ is not normal (e.g., highly skewed, uniform, etc.), the distribution of $\\bar{X}_n$ will approach a normal distribution due to the CLT.\n",
    "\n",
    "2. Larger Sample Size = Better Approximation:\n",
    "   - The approximation of $\\bar{X}_n \\sim \\mathcal{N}(\\mu, \\sigma^2 / n)$ improves as $n$ increases.\n",
    "\n",
    "\n",
    "\n",
    "Further explanations : https://www.probabilitycourse.com/chapter7/7_1_2_central_limit_theorem.php\n",
    "\n",
    "Youtube video : https://youtu.be/YAlJCEDH2uY?si=fm4lH1vTmX4-SWbV\n",
    "\n",
    "---\n",
    "\n",
    "### Applications\n",
    "\n",
    "The CLT is the foundation of statistical inference.\n",
    "\n",
    "In Machine learning, it is used to evaluate the performance of models.  \n",
    "For example, when comparing the performance of different models, you can use the CLT to calculate the standard error of the estimate, which helps to determine if the difference in performance is statistically significant.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1 : Experiment the CLT\n",
    "\n",
    "- Generate a population which is not normally distributed. Use numpy documentation to use the distribution you want : https://numpy.org/doc/2.1/reference/random/index.html\n",
    "\n",
    "- Observe the distribution of your generated data (please use seaborn instead of matplotlib for graphics : https://seaborn.pydata.org/generated/seaborn.histplot.html)\n",
    "\n",
    "- Create a code/procedure to observe the Central Limit Theorem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Law of Large Numbers (LLN)\n",
    "\n",
    "The **Law of Large Numbers (LLN)** in probability theory states that as the sample size increases, the sample average approaches the population mean. \n",
    "\n",
    "It has two main forms: **Weak LLN** and **Strong LLN**.\n",
    "\n",
    "\n",
    "\n",
    "### Weak Law of Large Numbers (WLLN)\n",
    "\n",
    "The sample mean **converges in probability** to the population mean $\\mu$ as the sample size $n$ increases:\n",
    "\n",
    "$$\n",
    "\\overline{X}_n \\xrightarrow{P} \\mu, \\quad \\text{as } n \\to \\infty.\n",
    "$$\n",
    "\n",
    "This means that for any small positive number $\\varepsilon > 0$ :\n",
    "\n",
    "$$\n",
    "\\lim_{n \\to \\infty} P(|\\overline{X}_n - \\mu| > \\varepsilon) = 0.\n",
    "$$\n",
    "\n",
    "#### Intuition\n",
    "\n",
    "The probability that the sample average $\\overline{X}_n$ differs from the true population mean $\\mu$ by more than a small margin $\\varepsilon$ becomes negligible as  $n \\to \\infty$. \n",
    "\n",
    "In practical terms, the sample averages get closer to the true mean as the sample size increases, though deviations may occasionally occur.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Strong Law of Large Numbers (SLLN)\n",
    "\n",
    "The strong LLN strengthens this result by stating that the sample mean **\"almost surely converges\"** to the population mean $\\mu$:\n",
    "\n",
    "$$\n",
    "\\overline{X}_n \\xrightarrow{a.s.} \\mu, \\quad \\text{as } n \\to \\infty.\n",
    "$$\n",
    "\n",
    "i.e.\n",
    "\n",
    "$$\n",
    "P\\Big(\\lim_{n \\to \\infty} \\overline{X}_n = \\mu\\Big) = 1.\n",
    "$$\n",
    "\n",
    "#### Intuition\n",
    "\n",
    "While the weak LLN suggests that  $\\overline{X}_n$  is \"very likely\" to approach  $\\mu$ , the strong LLN guarantees that  $\\overline{X}_n$  converges to  $\\mu$  with probability 1, meaning the deviations between  $\\overline{X}_n$  and  $\\mu$ \"disappear almost surely\".\n",
    "\n",
    "---\n",
    "\n",
    "## Importance of LLN in Machine Learning\n",
    "\n",
    "1. **Generalization**: The LLN ensures that as we gather more data, sample averages (e.g., empirical loss, accuracy) converge to the true population averages. This helps assess a model's generalization ability.\n",
    "\n",
    "2. **Monte Carlo Methods**: Many ML algorithms rely on sampling techniques (e.g., stochastic gradient descent) to estimate parameters. LLN guarantees these estimations stabilize with more samples, leading to reliable outcomes.\n",
    "\n",
    "\n",
    "In summary, the **Law of Large Numbers** provides theoretical confidence in ML when working with finite datasets, ensuring that performance evaluations and statistical estimates stabilize and reflect the true system's behavior as data size increases.\n",
    "\n",
    "Link for further explanations : https://www.probabilitycourse.com/chapter7/7_1_1_law_of_large_numbers.php\n",
    "\n",
    "Youtube Video : https://www.youtube.com/watch?v=ycuPP72_DVU \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Observe the Law of Large Numbers\n",
    "\n",
    "\n",
    "- Generate a population of any distribution of your choice using NumPy (https://numpy.org/doc/2.1/reference/random/index.html).\n",
    "- Find a way to experiment the LLN. \n",
    "Hint, you will need to randomly draw sub-samples from the population you generated. Ask yourself, what should you do with these sub-samples ?\n",
    "- Use Seaborn to visualize the convergence (https://seaborn.pydata.org/generated/seaborn.lineplot.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
