{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "36UyN4mn9kNa"
   },
   "source": [
    "# Assignment: Implementing Gradient Descent and Stochastic Gradient Descent\n",
    "\n",
    "## Understanding Different Types of Gradient Descent\n",
    "### Stochastic Gradient Descent (SGD)\n",
    "Stochastic Gradient Descent updates the model parameters using only one randomly chosen data point per iteration. \n",
    "\n",
    "This makes it computationally efficient and allows it to escape local minima, but it can be noisy and less stable compared to other methods. \n",
    "\n",
    "The randomness in the updates introduces variance, which can sometimes help in finding a better solution, but may also slow down convergence or cause oscillations.\n",
    "\n",
    "Mathematically, the update rule for SGD is:\n",
    "$$ \\theta^{(t+1)} = \\theta^{(t)} - \\alpha \\nabla J_i(\\theta) $$\n",
    "where:\n",
    "- $\\theta$ represents the parameters\n",
    "- $\\alpha$ is the learning rate\n",
    "- $\\nabla J_i(\\theta)$ is the gradient computed using a single data point $$ i $$\n",
    "\n",
    "\n",
    "### Mini-Batch Gradient Descent\n",
    "Mini-Batch Gradient Descent is a compromise between batch and stochastic gradient descent. Instead of using the entire dataset or a single data point, it updates the parameters using a small batch of randomly selected data points. This balances stability and efficiency, making it a commonly used approach in deep learning. Mini-batch gradient descent helps smooth out some of the noise of SGD while still being computationally efficient. The batch size is a hyperparameter that affects convergence speed and accuracy.\n",
    "\n",
    "The update rule for mini-batch gradient descent is:\n",
    "$$ \\theta^{(t+1)} = \\theta^{(t)} - \\alpha \\frac{1}{m} \\sum_{i=1}^{m} \\nabla J_i(\\theta) $$\n",
    "where:\n",
    "- $m$ is the mini-batch size\n",
    "- $\\sum_{i=1}^{m} \\nabla J_i(\\theta)$ represents the gradient computed over a batch of $m$ data points\n",
    "\n",
    "### Batch Gradient Descent (Vanilla)\n",
    "Batch Gradient Descent computes the gradient using the entire dataset at each iteration. \n",
    "\n",
    "While this method leads to a smooth convergence, it is computationally expensive, especially for large datasets. \n",
    "\n",
    "It tends to be more stable and can find the optimal solution efficiently, but it may take longer compared to SGD and Mini-Batch Gradient Descent.\n",
    "\n",
    "The update rule for batch gradient descent is:\n",
    "$$ \\theta^{(t+1)} = \\theta^{(t)} - \\alpha \\nabla J(\\theta) $$\n",
    "where:\n",
    "- $\\nabla J(\\theta)$ is the gradient computed over the entire dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n03T392ls9Li"
   },
   "source": [
    "## Task 1: Implement Gradient Descent for a Quadratic Function\n",
    "1. Define a quadratic function, e.g., $$ f(x) = x^2 $$.\n",
    "2. Implement a gradient descent to minimize this function.\n",
    "3. Use different learning rates and visualize the convergence.\n",
    "4. Plot the function and the gradient descent path.\n",
    "5. Plot the evolution of loss across iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YfUXGbh6uG6i"
   },
   "source": [
    "## Task 2: Gradient Descent in Higher Dimensions\n",
    "1. Define a function like $$ f(x, y) = x^2 + y^2 $$.\n",
    "2. Implement gradient descent for two variables.\n",
    "3. Visualize the contour plot and the optimization path.\n",
    "4. Experiment with different initialization points and learning rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y1-zlbxVuWXX"
   },
   "source": [
    "## Task 3: Implement Stochastic Gradient Descent (SGD)\n",
    "1. Generate a synthetic dataset : $$ y = 3x + 5 + \\text{noise} $$.\n",
    "2. Run gradient descent for linear regression.\n",
    "3. Run stochastic gradient descent and compare results.\n",
    "4. Plot the regression line and analyze the difference in convergence speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JVcJ9hRmuNWm"
   },
   "source": [
    "### Task 4\n",
    "1. Define the multimodal function $$f(x, y) = (x^2 + y^2) * sin(x) * sin(y) + x^2 + y^2$$.\n",
    "2. Visualize the 3D surface plot as well as the contour plot.\n",
    "3. What could be a problem for convergence of the algorithm here ?\n",
    "5. Run the gradient descent algorithm with different initial values and learning rates.\n",
    "7. Add the optimization path to the contour plot for each initial value.\n",
    "8. Compare the performance path of gradient descent and stochastic gradient descent for each initial value.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SKlearn regressions library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Take one dataset we already used for previous weeks. \n",
    "- Prepare data to train a model\n",
    "- Fit a linear regression with OLS method\n",
    "- Fit a linear regression with gradient descent (look at sklearn documentation to do so)\n",
    "- Compare and explain the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
